{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Market Basket Analysis with Spark FPGrowth (Data mining)\n",
        "#### Dataset download > \n",
        "* #### [Instacart](https://www.kaggle.com/c/instacart-market-basket-analysis)\n",
        "\n",
        "#### Library used >\n",
        "* #### [FPGrowth](https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#fp-growth)\n",
        "\n",
        "Mining frequent items, itemsets, subsequences, or other substructures is usually among the first steps to analyze a large-scale dataset, which has been an active research topic in data mining for years. We refer users to Wikipediaâ€™s association rule learning for more information. spark.mllib provides a parallel implementation of FP-growth, a popular algorithm to mining frequent itemsets.\n",
        "\n",
        "Market basket analysis may provide the retailer with information to understand the purchase behavior of a buyer. This information will enable the retailer to understand the buyer's needs and rewrite the store's layout accordingly, develop cross-promotional programs, or even capture new buyers (much like the cross-selling concept). An apocryphal early illustrative example for this was when one super market chain discovered in its analysis that male customers that bought diapers often bought beer as well, have put the diapers close to beer coolers, and their sales increased dramatically. Although this urban legend is only an example that professors use to illustrate the concept to students, the explanation of this imaginary phenomenon might be that fathers that are sent out to buy diapers often buy a beer as well, as a reward. This kind of analysis is supposedly an example of the use of data mining. A widely used example of cross selling on the web with market basket analysis is Amazon.com's use of \"customers who bought book A also bought book B\", e.g. \"People who read History of Portugal were also interested in Naval History\".\n",
        "\n",
        "This is a series of two notebooks. This is notebook #1. The purpose of this notebook is to prepare the dataset."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a31b69ac-94f3-4d37-9c55-1fadd1049d8c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data engineering pipelines\r\n",
        "Data engineering pipelines are commonly comprised of these components:\r\n",
        "![image-alt-text](https://s3.us-east-2.amazonaws.com/databricks-dennylee/media/data-engineering-pipeline-3.png)\r\n",
        "\r\n",
        "- Ingest Data: Bringing in the data from your source systems; often involving ETL processes (though we will skip this step in this demo for brevity)\r\n",
        "- Explore Data: Now that you have cleansed data, explore it so you can get some business insight\r\n",
        "- Train ML Model: Execute FP-growth for frequent pattern mining\r\n",
        "- Review Association Rules: Review the generated association rules"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import window as w "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7129051Z",
              "session_start_time": "2022-06-25T07:24:53.7498841Z",
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6163356Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Import Required Libraries",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "8a90bb8d-d0e0-40e5-a178-2736cf06b844"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading csv files in a dataframe\r\n",
        "file_path = \"abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/aisles/aisles.csv\"\r\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7141476Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6113472Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest Data\n",
        "\n",
        "The basic building block of the collaborative filter is transactional data containing a customer identifier. The popular [Instacart dataset](https://www.kaggle.com/c/instacart-market-basket-analysis) provides us a nice collection of such data with over 3 million grocery orders placed by over 200,000 Instacart users over a nearly 2-year period across of portfolio of nearly 50,000 products. \n",
        "\n",
        "**NOTE** Due to the terms and conditions by which these data are made available, anyone interested in recreating this work will need to download the data files from Kaggle and upload them to a folder structure as described below.\n",
        "\n",
        "The primary data files available for download are organized as follows under a pre-defined [mount point](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) that we have named */mnt/instacart*:\n",
        "\n",
        "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/instacart_filedownloads.png' width=250>\n",
        "\n",
        "\n",
        "\n",
        "Read into dataframes, these files form the following data model which captures the products customers have included in individual transactions:\n",
        "\n",
        "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/instacart_schema2.png' width=300>\n",
        "\n",
        "We will apply minimal transformations to this data, persisting it to the Delta Lake format for speedier access:"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8e8ca031-4399-4a93-a9cd-4f6183415b92"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = spark.sql('CREATE DATABASE IF NOT EXISTS instacart')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7155518Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6121794Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Create Database",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "74260186-6eeb-415e-9d8a-46035f29188b"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The orders data is pre-divided into *prior* and *training* evaluation sets, where the *training* dataset represents the last order placed in the overall sequence of orders associated with a given customer.  The *prior* dataset represents those orders that proceed the *training* order.  In a previous set of notebooks built on this data, we relabeled the *prior* and *training* evaluation sets as *calibration* and *evaluation*, respectively, to better align terminology with how the data was being used.  Here, we will preserve the *prior* & *training* designations as this better aligns with our current modeling needs.\n",
        "\n",
        "We will add to this dataset a field, *days_prior_to_last_order*, which calculates the days from a given order to the order that represents the *training* instance. This field will help us when developing features around purchases taking place different intervals prior to the final order.  All other tables will be brought into the database without schema changes, simply converting the underlying format from CSV to delta lake for better query performance later:"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6be6b60d-396d-47b2-979f-2ef972ddf2ad"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the old table if needed\n",
        "_ = spark.sql('DROP TABLE IF EXISTS instacart.orders')\n",
        "\n",
        "# define schema for incoming data\n",
        "orders_schema = StructType([\n",
        "  StructField('order_id', IntegerType()),\n",
        "  StructField('user_id', IntegerType()),\n",
        "  StructField('eval_set', StringType()),\n",
        "  StructField('order_number', IntegerType()),\n",
        "  StructField('order_dow', IntegerType()),\n",
        "  StructField('order_hour_of_day', IntegerType()),\n",
        "  StructField('days_since_prior_order', FloatType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "orders = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/orders',\n",
        "      header=True,\n",
        "      schema=orders_schema\n",
        "      )\n",
        "  )\n",
        "\n",
        "# calculate days until final purchase \n",
        "win = (\n",
        "  w.Window.partitionBy('user_id').orderBy(f.col('order_number').desc())\n",
        "  )\n",
        "\n",
        "orders_enhanced = (\n",
        "    orders\n",
        "      .withColumn(\n",
        "        'days_prior_to_last_order', \n",
        "        f.sum('days_since_prior_order').over(win) - f.coalesce(f.col('days_since_prior_order'),f.lit(0))\n",
        "        ) \n",
        "  )\n",
        "\n",
        "# write data to delta\n",
        "(\n",
        "  orders_enhanced\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .option('overwriteSchema','true')\n",
        "    .save('abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/orders')\n",
        "  )\n",
        "\n",
        "# make accessible as spark sql table\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE instacart.orders\n",
        "  USING DELTA\n",
        "  LOCATION 'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/orders'\n",
        "  ''')\n",
        "\n",
        "# present the data for review\n",
        "display(\n",
        "  spark\n",
        "    .table('instacart.orders')\n",
        "    .orderBy('user_id','order_number')\n",
        "    .limit(10)\n",
        "  )"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7171676Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6128456Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Orders",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "b3f752ca-b326-44c2-b7a6-9fb6276517df"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the old table if needed\n",
        "_ = spark.sql('DROP TABLE IF EXISTS instacart.products')\n",
        "\n",
        "# define schema for incoming data\n",
        "products_schema = StructType([\n",
        "  StructField('product_id', IntegerType()),\n",
        "  StructField('product_name', StringType()),\n",
        "  StructField('aisle_id', IntegerType()),\n",
        "  StructField('department_id', IntegerType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "products = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "     'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/products',\n",
        "      header=True,\n",
        "      schema=products_schema\n",
        "      )\n",
        "  )\n",
        "\n",
        "# write data to delta\n",
        "(\n",
        "  products\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .option('overwriteSchema','true')\n",
        "    .save('abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/products')\n",
        "  )\n",
        "\n",
        "# make accessible as spark sql table\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE instacart.products\n",
        "  USING DELTA\n",
        "  LOCATION 'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/products'\n",
        "  ''')\n",
        "\n",
        "# present the data for review\n",
        "display(\n",
        "  spark.table('instacart.products').limit(10)\n",
        "  )"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7186165Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6134299Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Products",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "38ee2b03-caba-48e1-9c0f-46a8b16b9a9b"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the old table if needed\n",
        "_ = spark.sql('DROP TABLE IF EXISTS instacart.order_products')\n",
        "\n",
        "# define schema for incoming data\n",
        "order_products_schema = StructType([\n",
        "  StructField('order_id', IntegerType()),\n",
        "  StructField('product_id', IntegerType()),\n",
        "  StructField('add_to_cart_order', IntegerType()),\n",
        "  StructField('reordered', IntegerType())\n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "order_products = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/order_products',\n",
        "      header=True,\n",
        "      schema=order_products_schema\n",
        "      )\n",
        "  )\n",
        "\n",
        "# write data to delta\n",
        "(\n",
        "  order_products\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .option('overwriteSchema','true')\n",
        "    .save('abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/order_products')\n",
        "  )\n",
        "\n",
        "# make accessible as spark sql table\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE instacart.order_products\n",
        "  USING DELTA\n",
        "  LOCATION 'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/order_products'\n",
        "  ''')\n",
        "\n",
        "# present the data for review\n",
        "display(\n",
        "  spark.table('instacart.order_products').limit(10)\n",
        "  )"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7207377Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6140153Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Order Products",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "5322e224-1ff7-4c32-bc35-96477725436a"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the old table if needed\n",
        "_ = spark.sql('DROP TABLE IF EXISTS instacart.departments')\n",
        "\n",
        "# define schema for incoming data\n",
        "departments_schema = StructType([\n",
        "  StructField('department_id', IntegerType()),\n",
        "  StructField('department', StringType())  \n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "departments = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/departments',\n",
        "      header=True,\n",
        "      schema=departments_schema\n",
        "      )\n",
        "  )\n",
        "\n",
        "# write data to delta\n",
        "(\n",
        "  departments\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .option('overwriteSchema','true')\n",
        "    .save('abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/departments')\n",
        "  )\n",
        "\n",
        "# make accessible as spark sql table\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE instacart.departments\n",
        "  USING DELTA\n",
        "  LOCATION 'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/departments'\n",
        "  ''')\n",
        "\n",
        "# present the data for review\n",
        "display(\n",
        "  spark.table('instacart.departments').limit(10)\n",
        "  )"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.722508Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6145628Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Departments",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "f74e60aa-c172-477c-a3bd-1da5a378df56"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the old table if needed\n",
        "_ = spark.sql('DROP TABLE IF EXISTS instacart.aisles')\n",
        "\n",
        "# define schema for incoming data\n",
        "aisles_schema = StructType([\n",
        "  StructField('aisle_id', IntegerType()),\n",
        "  StructField('aisle', StringType())  \n",
        "  ])\n",
        "\n",
        "# read data from csv\n",
        "aisles = (\n",
        "  spark\n",
        "    .read\n",
        "    .csv(\n",
        "      'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/bronze/aisles',\n",
        "      header=True,\n",
        "      schema=aisles_schema\n",
        "      )\n",
        "  )\n",
        "\n",
        "# write data to delta\n",
        "(\n",
        "  aisles\n",
        "    .write\n",
        "    .format('delta')\n",
        "    .mode('overwrite')\n",
        "    .option('overwriteSchema','true')\n",
        "    .save('abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/aisles')\n",
        "  )\n",
        "\n",
        "# make accessible as spark sql table\n",
        "_ = spark.sql('''\n",
        "  CREATE TABLE instacart.aisles\n",
        "  USING DELTA\n",
        "  LOCATION 'abfss://recommender@salabcommercedatalake.dfs.core.windows.net/instacart/silver/aisles'\n",
        "  ''')\n",
        "\n",
        "# present the data for review\n",
        "display(\n",
        "  spark.table('instacart.aisles').limit(10)\n",
        "  )"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7248051Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.615081Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Aisles",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "b9fa5946-979c-414e-9eac-643443682611"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine Order Details\n",
        "\n",
        "With our data loaded, we will flatten our order details through a view.  This will make access to our data during feature engineering significantly easier:"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "385f89f1-0c9d-45d8-a1c3-ae54b9426dc0"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "DROP VIEW IF EXISTS instacart.order_details;\n",
        "\n",
        "CREATE VIEW instacart.order_details as\n",
        "  SELECT\n",
        "    a.eval_set,\n",
        "    a.user_id,\n",
        "    a.order_number,\n",
        "    a.order_id,\n",
        "    a.order_dow,\n",
        "    a.order_hour_of_day,\n",
        "    a.days_since_prior_order,\n",
        "    a.days_prior_to_last_order,\n",
        "    b.product_id,\n",
        "    c.aisle_id,\n",
        "    c.department_id,\n",
        "    b.reordered\n",
        "  FROM instacart.orders a\n",
        "  INNER JOIN instacart.order_products b\n",
        "    ON a.order_id=b.order_id\n",
        "  INNER JOIN instacart.products c\n",
        "    ON b.product_id=c.product_id;\n",
        "    \n",
        "SELECT *\n",
        "FROM instacart.order_details;"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2022-06-25T07:24:53.7321559Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2022-06-25T07:25:05.6156795Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a5439d54-43a2-4332-9f0a-15ab874838a6"
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}